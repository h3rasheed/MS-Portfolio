{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e35ef8",
   "metadata": {},
   "source": [
    "# DTSC-670 Foundations of Machine Learning\n",
    "## Assignment 3\n",
    "### Name: (Please Enter Your Name Before Submitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7b0ec",
   "metadata": {},
   "source": [
    "## Copyright & Academic Integrity Notice\n",
    "<span style=\"color:red\">The assignment materials provided are exclusively for students officially enrolled in the course and are intended solely for purposes associated with the course. It is strictly prohibited to distribute these materials to others. Students are expressly forbidden from uploading these documents, parts of this assignment, or solutions to any external platforms such as websites, GitHub repositories, or personal websites.</span>\n",
    "\n",
    "<span style=\"color:red\">By submitting your document to CodeGrade, you are acknowledging that you fully understand the Academic Integrity policy as outlined in both the Program Handbook and the course syllabus. All submitted work must be solely your own, and any form of collaboration is strictly prohibited. You must not seek solutions online or submit them to any external websites. At the end of the term, plagiarism tracking software will be used for this assignment. Violations of the Academic Integrity policy will result in failure on the assignment, failure in the class, and/or dismissal from the program.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10ec61",
   "metadata": {},
   "source": [
    "## Student Learning Objectives\n",
    "\n",
    "- Keep refining your data preparation skills, including building pipelines and employing column transformers.\n",
    "- Develop a custom transformer tailored to your data manipulation needs.\n",
    "- Further enhance your proficiency in the machine learning work flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72009cf",
   "metadata": {},
   "source": [
    "## CodeGrade\n",
    "This assignment will be automatically graded through CodeGrade, and you will have unlimited submission attempts. To ensure successful grading, please follow these instructions carefully: Name your notebook as `assignment_3.ipynb` before submission, as CodeGrade requires this specific filename for grading purposes. Additionally, make sure there are no errors in your notebook, as CodeGrade will not be able to grade it if errors are present. Before submitting, we highly recommend restarting your kernel and running all cells again to ensure that there will be no errors when CodeGrade runs your script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711a190",
   "metadata": {},
   "source": [
    "## Assignment Overview\n",
    "In this assignment, your focus will remain on honing your skills in using Scikit-Learn functions for data preprocessing, with the overarching objective of constructing a classification machine learning model for predicting whether an individual's income exceeds $50,000 based on specific features. A pivotal aspect of this assignment will involve implementing a custom transformer to manipulate your dataset effectively.  Custom transformers enable us to customize preprocessing steps, making it effortless to reuse the code whenever we encounter new data or data preprocessing needs.\n",
    "\n",
    "### Data\n",
    "This data comes from the 1994 Census database and is a widely known beginner friendly dataset in the machine learning community.  You may hear it referred to as the \"Adult\" or \"Census Income\" dataset.  It is often used for practice with data exploration and in testing classification models.  More information about the dataset can be found on [UCI Machine Learning's site](https://archive.ics.uci.edu/dataset/2/adult).  Please make sure that you are using the files from Brightspace as they have been changed for this assignment.\n",
    "\n",
    "The columns in the file are as follows:\n",
    "\n",
    "    - age : continuous.\n",
    "    - workclass : Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "    - fnlwgt : continuous. Individuals who share similar demographic characteristics should ideally have similar weightings. However, it's crucial to remember that this principle holds true only within each state. This limitation arises because the CPS sample comprises 51 state-specific samples, each with its unique probability of selection.\n",
    "    - education : Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "    - education_num : continuous.\n",
    "    - marital_status : Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "    - occupation : Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "    - relationship : Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "    - race : White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "    - sex : Female, Male.\n",
    "    - capital_gain : continuous.\n",
    "    - capital_loss : continuous.\n",
    "    - days_per_week : continuous. Days worked per week.\n",
    "    - hours_per_day : continuous. Hours worked per day.\n",
    "    - native_country : United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "    - income : <=50K, >50K (this is our label)\n",
    "\n",
    "### Assignment Instructions\n",
    "Walk through the rest of the assignment, completing the exercises as indicated.  As you read through the markdown comments, the provided code, and create your own code, think about how each section fits into the overall machine learning process.  \n",
    "\n",
    "Once you have completed all the tasks, you are ready to submit your assignment to CodeGrade for testing. Please restart your notebook's kernel and run your code from the beginning to ensure there are no error messages. Once you have verified that the code runs without any issues, submit your .ipynb notebook file to CodeGrade for evaluation. Your notebook should be called `assignment_3.ipynb`. You have unlimited attempts for this assignment.\n",
    "\n",
    "### Table of Contents \n",
    "1. [Standard Imports](#import)\n",
    "2. [Get the Data](#data)\n",
    "3. [Explore the Data](#explore)\n",
    "4. [Prepare the Data](#prepare)\n",
    "5. [Model Selection & Evaluation](#model_selection)\n",
    "6. [Classification Metrics](#metrics)\n",
    "7. [Final Model Evaluation](#final_model)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f1bb9",
   "metadata": {},
   "source": [
    "## Standard Imports<a name=\"import\"></a>\n",
    "Run the code block below to import your standard imports and setup the notebook for CodeGrade grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a81e16",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Do not change this option; This allows the CodeGrade auto grading to function correctly\n",
    "pd.set_option('display.max_columns', 20)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5c5dd",
   "metadata": {},
   "source": [
    "## Get the Data<a name=\"data\"></a>\n",
    "**Exercise 1:** In the code block below, import the `census_income.csv` file and call the DataFrame `census_income.` If you examine the CSV file, you'll observe that it contains \"?\" for missing values. To handle this, refer to the [Pandas read_csv() function documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) to identify and use the suitable parameter for encoding all \"?\" values as NA/NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee0ed9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>days_per_week</th>\n",
       "      <th>hours_per_day</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>186061</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>310152</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age workclass  fnlwgt     education  education_num      marital_status  \\\n",
       "0       90       NaN   77053       HS-grad              9             Widowed   \n",
       "1       82   Private  132870       HS-grad              9             Widowed   \n",
       "2       66       NaN  186061  Some-college             10             Widowed   \n",
       "3       54   Private  140359       7th-8th              4            Divorced   \n",
       "4       41   Private  264663  Some-college             10           Separated   \n",
       "...    ...       ...     ...           ...            ...                 ...   \n",
       "32556   22   Private  310152  Some-college             10       Never-married   \n",
       "32557   27   Private  257302    Assoc-acdm             12  Married-civ-spouse   \n",
       "32558   40   Private  154374       HS-grad              9  Married-civ-spouse   \n",
       "32559   58   Private  151910       HS-grad              9             Widowed   \n",
       "32560   22   Private  201490       HS-grad              9       Never-married   \n",
       "\n",
       "              occupation   relationship   race     sex  capital_gain  \\\n",
       "0                    NaN  Not-in-family  White  Female             0   \n",
       "1        Exec-managerial  Not-in-family  White  Female             0   \n",
       "2                    NaN      Unmarried  Black  Female             0   \n",
       "3      Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4         Prof-specialty      Own-child  White  Female             0   \n",
       "...                  ...            ...    ...     ...           ...   \n",
       "32556    Protective-serv  Not-in-family  White    Male             0   \n",
       "32557       Tech-support           Wife  White  Female             0   \n",
       "32558  Machine-op-inspct        Husband  White    Male             0   \n",
       "32559       Adm-clerical      Unmarried  White  Female             0   \n",
       "32560       Adm-clerical      Own-child  White    Male             0   \n",
       "\n",
       "       capital_loss  days_per_week  hours_per_day native_country income  \n",
       "0              4356            5.0            8.0  United-States  <=50K  \n",
       "1              4356            6.0            3.0  United-States  <=50K  \n",
       "2              4356            5.0            8.0  United-States  <=50K  \n",
       "3              3900            5.0            8.0  United-States  <=50K  \n",
       "4              3900            5.0            8.0  United-States  <=50K  \n",
       "...             ...            ...            ...            ...    ...  \n",
       "32556             0            5.0            8.0  United-States  <=50K  \n",
       "32557             0            5.0            8.0  United-States  <=50K  \n",
       "32558             0            5.0            8.0  United-States   >50K  \n",
       "32559             0            5.0            8.0  United-States  <=50K  \n",
       "32560             0            5.0            4.0  United-States  <=50K  \n",
       "\n",
       "[32561 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Path to your CSV file\n",
    "file_path = 'census_income.csv'  \n",
    "\n",
    "# Reading the CSV file, replacing \"?\" with NaN\n",
    "census_income = pd.read_csv(file_path, na_values=\"?\")\n",
    "\n",
    "# Displaying the first few rows of the dataframe to confirm successful import\n",
    "census_income\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8f575",
   "metadata": {},
   "source": [
    "Let's again begin by examining fundamental details about the dataset. First, we will review the columns, check the total count of non-null entries, and analyze the data types associated with each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a48f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             32561 non-null  int64  \n",
      " 1   workclass       30725 non-null  object \n",
      " 2   fnlwgt          32561 non-null  int64  \n",
      " 3   education       32561 non-null  object \n",
      " 4   education_num   32561 non-null  int64  \n",
      " 5   marital_status  32561 non-null  object \n",
      " 6   occupation      30718 non-null  object \n",
      " 7   relationship    32561 non-null  object \n",
      " 8   race            32561 non-null  object \n",
      " 9   sex             32561 non-null  object \n",
      " 10  capital_gain    32561 non-null  int64  \n",
      " 11  capital_loss    32561 non-null  int64  \n",
      " 12  days_per_week   32561 non-null  float64\n",
      " 13  hours_per_day   32561 non-null  float64\n",
      " 14  native_country  31978 non-null  object \n",
      " 15  income          32561 non-null  object \n",
      "dtypes: float64(2), int64(5), object(9)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# check basic info about dataset and notice missing values\n",
    "census_income.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d5e64",
   "metadata": {},
   "source": [
    "**Exercise 2:** Before diving deeper into the data, we should stop and create a training and a test set.\n",
    "1) Since we are trying to predict whether an individual earns over $50K, save the `income` column as a Series named `income_label`.\n",
    "2) Drop the `income` column from the `census_income` DataFrame and save the remaining columns as a DataFrame named `income_features`.\n",
    "3) Utilize Scikit-learn's `train_test_split` function, employing the `income_features` and `income_label` variables, to partition the data into a training set and a test set. Allocate 80% of the instances for training and 20% for testing. Set the random_state to 42 to ensure reproducibility of our results.  Assign the DataFrames the following names: `X_train`, `X_test`, `y_train`, and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1c1785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haneef\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (6513, 15)\n",
      "y_train shape: (26048,)\n",
      "y_test shape: (6513,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Save the 'income' column as a Series named income_label\n",
    "income_label = census_income['income']\n",
    "\n",
    "# Step 2: Drop the 'income' column from the DataFrame and save the remaining columns\n",
    "income_features = census_income.drop('income', axis=1)\n",
    "\n",
    "# Step 3: Use train_test_split to partition the data\n",
    "# 80% for training and 20% for testing, random_state set to 42 for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(income_features, income_label, test_size=0.2, random_state=42)\n",
    "\n",
    "#\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd3778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559ae4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359e61f",
   "metadata": {},
   "source": [
    "## Explore the Data<a name=\"explore\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c56ca",
   "metadata": {},
   "source": [
    "Rather than exploring the data together in this notebook, we recommend taking some time to review how someone else approached data exploration. You can find a valuable example in this [Kaggle notebook by user ADITI MULYE](https://www.kaggle.com/code/aditimulye/adult-income-dataset-from-scratch). This individual has done a thorough job of exploring the data using methods we may not have covered yet. Examining how others explore data is a valuable learning experience that I highly recommend you invest time in.  Please keep in mind that while their exploration is insightful, our dataset may differ, so not all aspects may align perfectly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfdffb",
   "metadata": {},
   "source": [
    "## Prepare the Data<a name=\"prepare\"></a>\n",
    "In this section, we will build a custom transformer to address specific data preparation needs. We'll also set up pipelines to automate the data preparation process and introduce a column transformer to apply transformations to numeric and categorical columns in your dataset.\n",
    "\n",
    "Let's begin by creating our custom transformer.  \n",
    "\n",
    "### Custom Transformer\n",
    "Create a custom transformer, similar to the custom transformer that we saw in the California House Prices module example.  Go back and review that code, making sure that you understand the various pieces, in order to more easily create this custom transformer.  \n",
    "\n",
    "**Exercise 3:** Create a custom transformer that takes the numerical columns from your data and performs the following transformations:\n",
    "1) You must name your custom transformer class `Assignment3Transformer` \n",
    "2) Your class should include an input parameter called `create_new_column` with a default value of `True` that performs the following two data preparation steps when its value is `True`, but skips these steps and just returns the DataFrame as is when you pass a value of `False`.\n",
    "   - Adds an attribute to the end of the numerical data (i.e. new last column) that is the result of the `days_per_week` column multiplied by the `hours_per_day` column.  We are creating this column to better compare the amount of hours worked between the individuals.\n",
    "   - Since they are not needed with the new column, delete the `days_per_week` and `hours_per_day` columns.\n",
    "   - Remember that you only want these two steps to occur when the `create_new_column` parameter is `True`.  Your custom transfomer will be tested in CodeGrade to make sure these steps are not ran when the `create_new_column` parameter is `False`.\n",
    "\n",
    "This transformer will be used in a pipeline. In that pipeline, an imputer will be run *before* this transformer. Keep in mind that the imputer will output an array, so **this transformer must be written to accept an array.**  This is very important and a cause of many errors that students encounter.  In other words, think about using NumPy in your transformer instead of Pandas.\n",
    "\n",
    "Additionally, this transformer will ONLY be given the numerical features of the data. The categorical features will be handled elsewhere in the full pipeline. This means that your code for this transformer **must reflect the absence of the categorical columns** when indexing data structures.  Again this is very important and a cause of the second most number of errors that students encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92855946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class Assignment3Transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, create_new_column=True):\n",
    "        self.create_new_column = create_new_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Ensure X is a numpy array\n",
    "        X = np.array(X)\n",
    "\n",
    "        if self.create_new_column:\n",
    "            days_per_week_index = X.shape[1] - 2\n",
    "            hours_per_day_index = X.shape[1] - 1\n",
    "\n",
    "            # Create a new column by multiplying 'days_per_week' and 'hours_per_day'\n",
    "            new_column = X[:, days_per_week_index] * X[:, hours_per_day_index]\n",
    "\n",
    "            # Remove the original 'days_per_week' and 'hours_per_day' columns\n",
    "            X = np.delete(X, [days_per_week_index, hours_per_day_index], axis=1)\n",
    "\n",
    "            # Append the new column to the end of the array\n",
    "            X = np.column_stack([X, new_column])\n",
    "\n",
    "        return X\n",
    "\n",
    "# Example usage of the transformer\n",
    "# transformer = Assignment3Transformer(create_new_column=True)\n",
    "# transformed_data = transformer.transform(some_numpy_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722efa1",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "**Exercise 4:** Create a pipeline for only the numeric data called `num_pipeline` that:\n",
    "\n",
    "1) Utilizes Scikit-Learn's `make_pipeline` function to generate a pipeline named `num_pipeline`.  \n",
    "2) Within this pipeline, begin by incorporating a `SimpleImputer` transformation using the `mean` strategy. Please note that this strategy employs the \"mean\" instead of the \"median\" as used in the previous assignment. You might be wondering why we are introducing this step, given that there are no missing data in the numerical columns at the moment. While this holds true for the current dataset, we cannot always guarantee that incoming data will be free of missing values. Therefore, it is advisable to prepare for this possibility as a best practice.\n",
    "3) Next, apply the custom `Assignment3Transformer` class to the data.    \n",
    "4) Finally, add a `StandardScalar` transformation into the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7941ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Create a pipeline for numeric data\n",
    "num_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"mean\"),  # Imputer with mean strategy\n",
    "    Assignment3Transformer(create_new_column=True),  # Custom transformer\n",
    "    StandardScaler()  # StandardScaler for normalization\n",
    ")\n",
    "\n",
    "# Now, num_pipeline can be used to process numerical data\n",
    "# Example usage: processed_data = num_pipeline.fit_transform(some_numeric_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635daa8",
   "metadata": {},
   "source": [
    "**Exercise 5:** Create a pipeline for only the categorical data called `cat_pipeline` that:\n",
    "\n",
    "1) Utilizes Scikit-Learn's `make_pipeline` function to generate a pipeline named `cat_pipeline`.  \n",
    "2) Begin by incorporating a `SimpleImputer` transformation using the `most_frequent` strategy.  This will replace any missing values with the most frequent value for each column.\n",
    "3) Next, add an `OneHotEncoder` class to the pipeline, making sure that the `drop` parameter is set to \"first\".  You must also include the `sparse_output=False` parameter to prevent a sparse array from being generated to make CodeGrade testing easier.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33133cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a pipeline for categorical data\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),  # Imputer with most_frequent strategy\n",
    "    OneHotEncoder(drop=\"first\", sparse=False)  # OneHotEncoder with specified parameters\n",
    ")\n",
    "\n",
    "# Now, cat_pipeline can be used to process categorical data\n",
    "# Example usage: processed_categorical_data = cat_pipeline.fit_transform(some_categorical_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc681c",
   "metadata": {},
   "source": [
    "### Column Transformer\n",
    "Next, you will create a Column Transformer to pass your numeric data to the `num_pipeline` and your categorical features to the `cat_pipeline` you created above.\n",
    "\n",
    "**Exercise 6:**\n",
    "1) Create a list of your numerical feature column names (in the order they appear in the original data). Name this list `num_attributes`.\n",
    "2) Create a list of your categorical feature column names (in the order they appear in the original data). Name this list `cat_attributes`.\n",
    "3) Utilize Scikit-learn's `ColumnTransformer` function to create a transformer that:\n",
    "    - Directs the numeric data through the previously defined `num_pipeline`.\n",
    "    - Directs the categorical features through the previously defined `cat_pipeline`.\n",
    "    - Name this ColumnTransformer object `preprocessing`.\n",
    "4) Invoke the fit_transform() method on the `X_train` dataset to generate the preprocessed data. Store the resulting output in a variable named `X_train_prepared`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51f8dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.92195464,  0.63253053, -0.42172678, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.06915047, -0.18615485, -0.42172678, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.18597545, -1.08543674,  1.12825935, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 1.212385  ,  0.46047387, -0.42172678, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.18201414, -0.34040696, -0.03423025, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.21634631, -0.25861078, -0.42172678, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# Step 1: List of numerical feature column names\n",
    "num_attributes = list(income_features.select_dtypes(include=[np.number]).columns)\n",
    "\n",
    "# Step 2: List of categorical feature column names\n",
    "cat_attributes = list(income_features.select_dtypes(exclude=[np.number]).columns)\n",
    "\n",
    "# Step 3: Create a ColumnTransformer\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attributes),\n",
    "    (\"cat\", cat_pipeline, cat_attributes)\n",
    "])\n",
    "\n",
    "# Step 4: Apply the ColumnTransformer to X_train\n",
    "X_train_prepared = preprocessing.fit_transform(X_train)\n",
    "\n",
    "# X_train_prepared now holds the preprocessed data\n",
    "X_train_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09629c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "163df77d",
   "metadata": {},
   "source": [
    "## Model Selection<a name=\"model_selection\"></a>\n",
    "In this section, we will employ various models on our preprocessed data and assess their accuracy scores using Scikit-Learn's `cross_val_score` function. Proceed by executing the following cell groups to fit models, including a Logistic Regression model, a Stochastic Gradient Descent classifier, and a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3125454b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Logistic Regression Classifier ###\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate a Logistic Regression Class \n",
    "# increasing the maximum number of iterations taken for the solvers to converge\n",
    "log_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# fit the model\n",
    "log_clf.fit(X_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0076bf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85108833, 0.85200967, 0.84991937])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# check the accuracy scores\n",
    "cross_val_score(log_clf, X_train_prepared, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b1170c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Stochastic Gradient Descent Classifier ###\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# instantiate SGD CLassifier Class\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "\n",
    "# fit the model \n",
    "sgd_clf.fit(X_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ab7cb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84717264, 0.84901532, 0.84842202])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy scores\n",
    "cross_val_score(sgd_clf, X_train_prepared, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82686fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Random Forest Classifier ###\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate a Random Forest Classifier Class using default parameters\n",
    "# we won't do it in this assignment, but normally we would want to perform a grid search to \n",
    "# find the best parameters to use\n",
    "rnd_for_clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# fit the model\n",
    "rnd_for_clf.fit(X_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "947f1f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8551192 , 0.85396752, 0.85418106])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy scores\n",
    "cross_val_score(rnd_for_clf, X_train_prepared, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449b310",
   "metadata": {},
   "source": [
    "The accuracy scores for all three models fall within the range of 84-85%. In a real-world project, the next step would involve fine-tuning the model's hyperparameters. However, for the purpose of this assignment, we will not delve into hyperparameter tuning. \n",
    "\n",
    "Given that the accuracy scores are quite similar among the models, we will proceed with the Logistic Regression model. As we have come to understand, accuracy alone doesn't provide a comprehensive assessment of classification tasks. To gain a more comprehensive insight, let's evaluate the precision, recall, and F1 scores for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0975f85",
   "metadata": {},
   "source": [
    "## Classification Metrics<a name=\"metrics\"></a>\n",
    "**Exercise 7:** \n",
    "1) Utilizing Scikit-Learn's [cross_val_predict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict) function, generate a list of predictions named `y_train_pred` by using the `log_clf` model, your `X_train_prepared` data, and your `y_train` data using 3-fold cross-validation.\n",
    "2) Next, calculate the precision score, recall score, and F1 score by employing the [precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score), [recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score), and [f1_score]((https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)) functions on the `y_train` and `y_train_pred` data, and store the results in variables named `precision`, `recall`, and `f1` respectively. It's important to note that you should specify `pos_label=\">50K\"` as a parameter in these functions to indicate the positive label.  Round these scores to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2212d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.74, 0.6, 0.66)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Generate a list of predictions using cross-validation\n",
    "y_train_pred = cross_val_predict(log_clf, X_train_prepared, y_train, cv=3)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_train, y_train_pred, pos_label=\">50K\")\n",
    "recall = recall_score(y_train, y_train_pred, pos_label=\">50K\")\n",
    "f1 = f1_score(y_train, y_train_pred, pos_label=\">50K\")\n",
    "\n",
    "# Round the scores to 2 decimal places\n",
    "precision = round(precision, 2)\n",
    "recall = round(recall, 2)\n",
    "f1 = round(f1, 2)\n",
    "\n",
    "precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae04e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12aab595",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26b95eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0f618",
   "metadata": {},
   "source": [
    "Take a moment to reflect on these metrics and their meanings. If an employer or interviewer were to inquire about them, consider how you would explain these metric scores to someone else. It's very important for a data scientist, or someone working in the field, to be able to convey a clear understanding of these metrics, including their significance in evaluating the performance of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da30c4",
   "metadata": {},
   "source": [
    "## Final Model Evaluation<a name=\"final_model\"></a>\n",
    "We are now ready to assess our model's performance on the test set, utilizing the previously established Logistic Regression model. It is necessary that we apply any data transformations we performed on the training data to the testing data. It is crucial to emphasize that we should solely apply transformations to the testing data without using the \"fit_transform\" method, as we want to exclusively use the information derived from the training data for this transformation process.\n",
    "\n",
    "**Exercise 8:**\n",
    "1) Utilizing the previously established `preprocessing` ColumnTransformer, apply transformations to your `X_test` data, and label the resulting dataset as `X_test_prepared`. It is essential that you should refrain from using the fit_transform method on your testing data in any capacity.\n",
    "2) With the pre-fitted `log_clf` model, make predictions using the `X_test_prepared` dataset and store these predictions as a variable called `final_predictions`.\n",
    "3) Utilizing Scikit-learn's [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function, calculate the accuracy by passing your `y_test` and `final_predictions`.  Round the accuracy score to 2 decimal places. Save this score as `final_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96e61edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Apply the transformations to X_test using the preprocessing ColumnTransformer\n",
    "X_test_prepared = preprocessing.transform(X_test)\n",
    "\n",
    "# Make predictions using the pre-fitted log_clf model\n",
    "final_predictions = log_clf.predict(X_test_prepared)\n",
    "\n",
    "# Calculate the accuracy\n",
    "final_accuracy = accuracy_score(y_test, final_predictions)\n",
    "\n",
    "# Round the accuracy score to 2 decimal places\n",
    "final_accuracy = round(final_accuracy, 2)\n",
    "\n",
    "final_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689124e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
