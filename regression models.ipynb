{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e35ef8",
   "metadata": {},
   "source": [
    "# DTSC-670 Foundations of Machine Learning\n",
    "## Assignment 5\n",
    "### Name: Haneefuddin Rsheed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7b0ec",
   "metadata": {},
   "source": [
    "## Copyright & Academic Integrity Notice\n",
    "<span style=\"color:red\">The assignment materials provided are exclusively for students officially enrolled in the course and are intended solely for purposes associated with the course. It is strictly prohibited to distribute these materials to others. Students are expressly forbidden from uploading these documents, parts of this assignment, or solutions to any external platforms such as websites, GitHub repositories, or personal websites.</span>\n",
    "\n",
    "<span style=\"color:red\">By submitting your document to CodeGrade, you are acknowledging that you fully understand the Academic Integrity policy as outlined in both the Program Handbook and the course syllabus. All submitted work must be solely your own, and any form of collaboration is strictly prohibited. You must not seek solutions online or submit them to any external websites. At the end of the term, plagiarism tracking software will be used for this assignment. Violations of the Academic Integrity policy will result in failure on the assignment, failure in the class, and/or dismissal from the program.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10ec61",
   "metadata": {},
   "source": [
    "## Student Learning Objectives\n",
    "\n",
    "- Evaluate different regression models and assess their performance relative to ridge regression regularization.\n",
    "- Explore the utility of ridge regression in mitigating multicollinearity concerns in the data.\n",
    "- Gain experience in employing grid search to identify optimal hyperparameters for a given model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72009cf",
   "metadata": {},
   "source": [
    "## CodeGrade\n",
    "This assignment will be automatically graded through CodeGrade, and you will have unlimited submission attempts. To ensure successful grading, please follow these instructions carefully: Name your notebook as `assignment_5.ipynb` before submission, as CodeGrade requires this specific filename for grading purposes. Additionally, make sure there are no errors in your notebook, as CodeGrade will not be able to grade it if errors are present. Before submitting, we highly recommend restarting your kernel and running all cells again to ensure that there will be no errors when CodeGrade runs your script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711a190",
   "metadata": {},
   "source": [
    "## Assignment Overview\n",
    "The objective of this assignment is to demonstrate that sometimes regularization techniques can help address multicollinearity problems in your dataset. You will gain experience in building diverse regression models and evaluating their performance in comparison to a ridge regression regularized model. Additionally, you will practice utilizing grid search to obtain the optimal hyperparameters for your models.\n",
    "\n",
    "### Data\n",
    "The data for this assignment is made-up and has been intentionally created in such a way that there is high correlation between predictor variables.  If your dataset includes predictor variables that exhibit strong correlations among themselves, ridge regression can sometimes be employed to alleviate multicollinearity by introducing a penalty term into the regression coefficients. This penalty term serves to restrain the coefficients, thus enhancing their stability.\n",
    "\n",
    "Please download the `ridge_reg_data.csv` file from Brightspace and put it in the same folder as this notebook.\n",
    "\n",
    "### Assignment Instructions\n",
    "Walk through the assignment and follow the directions as requested.  Once you have completed all the tasks, you are ready to submit your assignment to CodeGrade for testing. Please restart your notebook's kernel and run your code from the beginning to ensure there are no error messages. Once you have verified that the code runs without any issues, submit your .ipynb notebook file to CodeGrade for evaluation. Your notebook should be called `assignment_5.ipynb`. You have unlimited attempts for this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f1bb9",
   "metadata": {},
   "source": [
    "## Standard Imports<a name=\"import\"></a>\n",
    "Run the code block below to import your standard imports and setup the notebook for CodeGrade grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a81e16",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Do not change this option; This allows the CodeGrade auto grading to function correctly\n",
    "pd.set_option('display.max_columns', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc31f9",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "**Exercise 1:** Load the file named `ridge_reg_data.csv` and store its contents in a DataFrame named `ridge_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45edec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_data = pd.read_csv('ridge_reg_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36719efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.548814</td>\n",
       "      <td>0.715189</td>\n",
       "      <td>0.602763</td>\n",
       "      <td>0.544883</td>\n",
       "      <td>1.317953</td>\n",
       "      <td>4.232345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.645894</td>\n",
       "      <td>0.437587</td>\n",
       "      <td>0.891773</td>\n",
       "      <td>0.963663</td>\n",
       "      <td>1.329360</td>\n",
       "      <td>5.235632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.791725</td>\n",
       "      <td>0.528895</td>\n",
       "      <td>0.568045</td>\n",
       "      <td>0.925597</td>\n",
       "      <td>1.096939</td>\n",
       "      <td>5.245756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.087129</td>\n",
       "      <td>0.020218</td>\n",
       "      <td>0.832620</td>\n",
       "      <td>0.778157</td>\n",
       "      <td>0.852838</td>\n",
       "      <td>2.990405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.978618</td>\n",
       "      <td>0.799159</td>\n",
       "      <td>0.461479</td>\n",
       "      <td>0.780529</td>\n",
       "      <td>1.260638</td>\n",
       "      <td>6.951112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.377716</td>\n",
       "      <td>0.918172</td>\n",
       "      <td>0.579258</td>\n",
       "      <td>0.369722</td>\n",
       "      <td>1.497429</td>\n",
       "      <td>6.335631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.924504</td>\n",
       "      <td>0.673493</td>\n",
       "      <td>0.230904</td>\n",
       "      <td>0.819261</td>\n",
       "      <td>0.904397</td>\n",
       "      <td>5.958141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.698047</td>\n",
       "      <td>0.486354</td>\n",
       "      <td>0.940865</td>\n",
       "      <td>0.068375</td>\n",
       "      <td>1.427219</td>\n",
       "      <td>7.331734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.194568</td>\n",
       "      <td>0.221001</td>\n",
       "      <td>0.235428</td>\n",
       "      <td>0.152850</td>\n",
       "      <td>0.456428</td>\n",
       "      <td>3.313825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.218156</td>\n",
       "      <td>0.235453</td>\n",
       "      <td>0.197388</td>\n",
       "      <td>0.398687</td>\n",
       "      <td>0.432842</td>\n",
       "      <td>2.243334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        col_1     col_2     col_3     col_4     col_5    target\n",
       "0    0.548814  0.715189  0.602763  0.544883  1.317953  4.232345\n",
       "1    0.645894  0.437587  0.891773  0.963663  1.329360  5.235632\n",
       "2    0.791725  0.528895  0.568045  0.925597  1.096939  5.245756\n",
       "3    0.087129  0.020218  0.832620  0.778157  0.852838  2.990405\n",
       "4    0.978618  0.799159  0.461479  0.780529  1.260638  6.951112\n",
       "..        ...       ...       ...       ...       ...       ...\n",
       "995  0.377716  0.918172  0.579258  0.369722  1.497429  6.335631\n",
       "996  0.924504  0.673493  0.230904  0.819261  0.904397  5.958141\n",
       "997  0.698047  0.486354  0.940865  0.068375  1.427219  7.331734\n",
       "998  0.194568  0.221001  0.235428  0.152850  0.456428  3.313825\n",
       "999  0.218156  0.235453  0.197388  0.398687  0.432842  2.243334\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the data\n",
    "ridge_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed36f6",
   "metadata": {},
   "source": [
    "**Exercise 2:** Let's create a training and a test set.\n",
    "1) Drop the `target` column from the `ridge_data` DataFrame and store the remaining features in a DataFrame named `X` (*uppercase X*).\n",
    "2) Save the `target` column as a Series named `y` (*lowercase y*).\n",
    "3) Utilize Scikit-learn's `train_test_split` function, using `X` and `y`to create a training set and a test set. Allocate 80% of the instances for training and 20% for testing. Set the random_state to 42 to ensure reproducibility of our results.  Assign the DataFrames the following names: `X_train`, `X_test`, `y_train`, and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb95afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haneef\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Drop the target column and store the remaining features in X\n",
    "X = ridge_data.drop('target', axis=1)\n",
    "\n",
    "# Save the target column as a Series y\n",
    "y = ridge_data['target']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909905f",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares Linear Regression\n",
    "**Exercise 3:** \n",
    "1) Create an instance of Scikit-Learn's `LinearRegression()` class and name this object `ols_model`.\n",
    "2) Train your `ols_model` by fitting it with the `X_train` and `y_train` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf86d168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Step 1: Create an instance of LinearRegression\n",
    "ols_model = LinearRegression()\n",
    "\n",
    "# Step 2: Train the model with X_train and y_train\n",
    "ols_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1316c616",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "**Exercise 4:** \n",
    "1) Create an instance of Scikit-Learn's `SGDRegressor()` class and name this object `sgd_model`.  Include the following hyperparameters:\n",
    "    - max_iter = 10000\n",
    "    - penalty = None\n",
    "    - learning_rate = 'constant'\n",
    "    - n_iter_no_change = 100\n",
    "    - random_state = 42\n",
    "    - Take some time to look at [Scikit-Learn's documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) to understand the definitions of these hyperparameters.  You can also play around with different hyperparameter values on your own (just please do not include your work in this notebook when you submit it) to see how the model changes.\n",
    "2) Note: Do not train your model yet.  We will do that in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c023e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Create an instance of SGDRegressor with specified hyperparameters\n",
    "sgd_model = SGDRegressor(max_iter=10000,\n",
    "                         penalty=None,\n",
    "                         learning_rate='constant',\n",
    "                         n_iter_no_change=100,\n",
    "                         random_state=42)\n",
    "\n",
    "# Note: Model is not trained yet, it will be trained in the next section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65d04b7",
   "metadata": {},
   "source": [
    "### Grid Search for SGDRegressor Model\n",
    "In the module videos, we discussed the importance of the initial learning rate (called `eta0` in sklearn) and stopping criterion (called `tol` in sklearn).  We've created a simple grid search below that:\n",
    "1) Creates a dictionary of options for `eta0` and `tol` \n",
    "2) Instantiates a `GridSearchCV()` class that passes the `sgd_model` that you created earlier along with the `sgd_param_grid` dictionary.  Take a look at [Scikit-Learn's documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to understand the `verbose` and `cv` parameters.   \n",
    "3) Fits your grid search using your `X_train` and `y_train` data and prints out the best parameters found during the grid search.\n",
    "4) Saves the best model from the grid search as `sgd_model`.\n",
    "\n",
    "Run the code block below to run through the steps outlined above and make sure that you understand what is going on in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6418e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "The best parameters are:  {'eta0': 0.001, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create a dictionary of hyperparameter values\n",
    "sgd_param_grid = {'eta0': [.1, .01, .001],\n",
    "                  'tol': [1e-1, 1e-3, 1e-5, 1e-7]}\n",
    "\n",
    "# instantiate GridSearchCV()\n",
    "grid_search_cv_sgd = GridSearchCV(sgd_model,\n",
    "                              sgd_param_grid, verbose=1, cv=10)\n",
    "\n",
    "# fits the grid search model and printes the best parameters\n",
    "grid_search_cv_sgd.fit(X_train, y_train)\n",
    "print(\"The best parameters are: \", grid_search_cv_sgd.best_params_)\n",
    "\n",
    "# saves the best model using the hyperparameters from the grid search\n",
    "sgd_model = grid_search_cv_sgd.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cc3316",
   "metadata": {},
   "source": [
    "It can be beneficial to conduct additional rounds of grid searches to explore non-searched areas of the hyperparameter space. For instance, if the optimal `eta0` parameter was found to be 0.001, it might be worthwhile to explore values both below and above that threshold to determine if further model enhancements can be achieved. However, we will not perform further grid searches for this assignment as they are unlikely to yield significant benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4dd8fa",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "As we mentioned in the data description, this dataset was specifically created with high correlation between predictor values.  When working with data that contains multicollinearity issues, ridge regression can sometimes help to mitigate this problem.\n",
    "\n",
    "**Exercise 5:** \n",
    "1) Create an instance of Scikit-Learn's `RidgeRegression()` class and name this object `ridge_model`.  Set your `random_state` to 42.\n",
    "2) Note: Do not train your model yet.  We will do that in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "147b8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create an instance of Ridge Regression with random_state set to 42\n",
    "ridge_model = Ridge(random_state=42)\n",
    "\n",
    "# Note: The model is not trained yet, it will be trained in the next section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421482b5",
   "metadata": {},
   "source": [
    "### Grid Search for Ridge Regression Model\n",
    "In this grid search, we will search for the best `alpha` value to use in our `ridge_model`.  Remember from the module that alpha (sometimes called lambda) represents the amount of penalty added to the traditional least squares method.\n",
    "    \n",
    "**Exercise 6:** Similar to how the grid search was performed with the SGDRegressor model:\n",
    "1) Create a dictionary of values for `alpha` and call this dictionary `ridge_param_grid`.  The values of alpha that you will search are: 0.25, 0.50, 0.75, 1, 2, 3, 5, 10, 100\n",
    "2) Instantiate the `GridSearchCV()` class and pass it your `ridge_model` and your `ridge_param_grid` dictionary.  Use `verbose=1` and `cv=10`.\n",
    "3) Fit your grid search model using the `X_train` and `y_train`.\n",
    "4) Save the best model from the grid search as `ridge_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0947ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "The best parameters are:  {'alpha': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1) Create a dictionary of values for alpha\n",
    "ridge_param_grid = {'alpha': [0.25, 0.50, 0.75, 1, 2, 3, 5, 10, 100]}\n",
    "\n",
    "# 2) Instantiate GridSearchCV\n",
    "grid_search_cv_ridge = GridSearchCV(ridge_model,\n",
    "                                    ridge_param_grid, \n",
    "                                    verbose=1, \n",
    "                                    cv=10)\n",
    "\n",
    "# 3) Fit the grid search model using X_train and y_train\n",
    "grid_search_cv_ridge.fit(X_train, y_train)\n",
    "\n",
    "# 4) Save the best model from the grid search as ridge_model\n",
    "ridge_model = grid_search_cv_ridge.best_estimator_\n",
    "\n",
    "# Optionally, you can print the best parameters\n",
    "print(\"The best parameters are: \", grid_search_cv_ridge.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4cc3a2",
   "metadata": {},
   "source": [
    "## Calculate Metrics\n",
    "Let's now see how the three models compare using the root mean squared error (RMSE) and R-squared metrics.\n",
    "\n",
    "**Exercise 7:** \n",
    "1) Using your `ols_model`, `sgd_model`, and `ridge_model` make predictions using your `X_test` data and save these predictions as `ols_pred`, `sgd_pred`, and `ridge_pred` respectively.\n",
    "2) Find the RMSE for each model using Scikit-Learn's `mean_squared_error` function passing it your `y_test` and predictions from step 1 above.  Round these RMSE scores to 4 decimal places.  Save these scores as `rmse_ols`, `rmse_sgd`, and `rmse_ridge` respectively.  Hint: Remember that we are asking for RMSE and not MSE.  Check Scikit-Learn's documentation for how to calculate the RMSE value.\n",
    "3) Find the R-squared values for each model using Scikit-Learn's `r2_score` function passing it your `y_test` and predictions from step 1 above.  Round these R-squared scores to 4 decimal places.  Save these scores as `r2_ols`, `r2_sgd`, and `r2_ridge` respectively.\n",
    "4) Print the MSE and R-squared scores for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b74585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ols_model to make predictions\n",
    "ols_pred = ols_model.predict(X_test)\n",
    "\n",
    "# Using sgd_model to make predictions\n",
    "sgd_pred = sgd_model.predict(X_test)\n",
    "\n",
    "# Using ridge_model to make predictions\n",
    "ridge_pred = ridge_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe89ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculating RMSE for ols_model\n",
    "rmse_ols = np.sqrt(mean_squared_error(y_test, ols_pred))\n",
    "rmse_ols = round(rmse_ols, 4)\n",
    "\n",
    "# Calculating RMSE for sgd_model\n",
    "rmse_sgd = np.sqrt(mean_squared_error(y_test, sgd_pred))\n",
    "rmse_sgd = round(rmse_sgd, 4)\n",
    "\n",
    "# Calculating RMSE for ridge_model\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, ridge_pred))\n",
    "rmse_ridge = round(rmse_ridge, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a0d8427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Model - RMSE: 0.9869, R-squared: 0.6741\n",
      "SGD Model - RMSE: 0.9853, R-squared: 0.6752\n",
      "Ridge Model - RMSE: 0.9841, R-squared: 0.676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculating R-squared for ols_model\n",
    "r2_ols = r2_score(y_test, ols_pred)\n",
    "r2_ols = round(r2_ols, 4)\n",
    "\n",
    "# Calculating R-squared for sgd_model\n",
    "r2_sgd = r2_score(y_test, sgd_pred)\n",
    "r2_sgd = round(r2_sgd, 4)\n",
    "\n",
    "# Calculating R-squared for ridge_model\n",
    "r2_ridge = r2_score(y_test, ridge_pred)\n",
    "r2_ridge = round(r2_ridge, 4)\n",
    "\n",
    "# Printing the MSE and R-squared scores for each model\n",
    "print(f\"OLS Model - RMSE: {rmse_ols}, R-squared: {r2_ols}\")\n",
    "print(f\"SGD Model - RMSE: {rmse_sgd}, R-squared: {r2_sgd}\")\n",
    "print(f\"Ridge Model - RMSE: {rmse_ridge}, R-squared: {r2_ridge}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48bec636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (OLS): 0.9869\n",
      "RMSE (SGD): 0.9853\n",
      "RMSE (Ridge): 0.9841\n",
      "----------\n",
      "R-squared (OLS): 0.6741\n",
      "R-squared (SGD): 0.6752\n",
      "R-squared (Ridge): 0.676\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE (OLS): {rmse_ols}\")\n",
    "print(f\"RMSE (SGD): {rmse_sgd}\")\n",
    "print(f\"RMSE (Ridge): {rmse_ridge}\")\n",
    "print(\"----------\")\n",
    "print(f\"R-squared (OLS): {r2_ols}\")\n",
    "print(f\"R-squared (SGD): {r2_sgd}\")\n",
    "print(f\"R-squared (Ridge): {r2_ridge}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f0394",
   "metadata": {},
   "source": [
    "It's worth noting that the ridge regression model demonstrated a small enhancement compared to both the ordinary least squares and stochastic gradient descent models. Naturally, it's important to assess whether these differences hold statistical significance. Additionally, the selection of different hyperparameter values can significantly influence the final model outcome.\n",
    "\n",
    "While ridge regression may not consistently help with multicollinearity, it's typically a technique that is worthy to explore to see if there are potential improvements in the model."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
